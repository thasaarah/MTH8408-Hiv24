{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 3: Optimisation sans contraintes et méthodes itératives\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3`"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "project..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff → ForwardDiffStaticArraysExt\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  ✓ \u001b[39m\u001b[90mReverseDiff\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m  ✓ \u001b[39mADNLPModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3 dependencies successfully precompiled in 37 seconds. 235 already precompiled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `C:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Project.toml`\n",
      "  \u001b[90m[54578032] \u001b[39mADNLPModels v0.7.0\n",
      "  \u001b[90m[ecbce9bc] \u001b[39mBenchmarkProfiles v0.4.4\n",
      "  \u001b[90m[6e4b80f9] \u001b[39mBenchmarkTools v1.4.0\n",
      "  \u001b[90m[7073ff75] \u001b[39mIJulia v1.24.2\n",
      "  \u001b[90m[10dff2fc] \u001b[39mJSOSolvers v0.11.0\n",
      "  \u001b[90m[5c8ed15e] \u001b[39mLinearOperators v2.6.0\n",
      "  \u001b[90m[a4795742] \u001b[39mNLPModels v0.20.0\n",
      "  \u001b[90m[792afdf1] \u001b[39mNLPModelsJuMP v0.12.5\n",
      "  \u001b[90m[5049e819] \u001b[39mOptimizationProblems v0.7.3\n",
      "  \u001b[90m[581a75fa] \u001b[39mSolverBenchmark v0.6.0\n",
      "  \u001b[90m[ff4d7338] \u001b[39mSolverCore v0.3.7\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[de0858da] \u001b[39mPrintf\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") #Accède au fichier Project.toml\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  EmptyADbackend,\n",
       "  ForwardDiffADHessian,\n",
       "  EmptyADbackend,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test problem:\n",
    "using ADNLPModels\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "problem2 = ADNLPModel(x->-x[1]^2, ones(3))\n",
    "\n",
    "roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires sur Julia\n",
    "\n",
    "Quelques commentaires sur des morceaux de codes que vous avez vu:\n",
    "- les structures, exemple [GenericExecutionStats](https://github.com/JuliaSmoothOptimizers/SolverCore.jl/blob/0091f437a26a27ac8aa53d5e37647223722f7f7c/src/stats.jl#L60) (constructeur, attribut, type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? GenericExecutionStats\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? GenericExecutionStats\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? GenericExecutionStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les arguments dans les fonctions. Lire attentivement [la documentation Julia sur les fonctions](https://docs.julialang.org/en/v1/manual/functions/) pour comprendre l'utilisation des `Optional Arguments` et des `Keywords Arguments`. Ce type d'arguments est très utile dans nos applictions où les solveurs dépendent de paramètre dont on peut fixer des valeurs par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Méthode BFGS avec mémoire limitée (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode BFGS à mémoire limitée vue en cours en utilisant les `InverseLBFGSOperator` du package `LinearOperators.jl`. Il y a aussi un petit exemple dans la documentation du package [LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1](https://juliasmoothoptimizers.github.io/LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? InverseLBFGSOperator\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? InverseLBFGSOperator\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? InverseLBFGSOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui est important dans ce type de méthode est:\n",
    "- le paramètre mémoire\n",
    "- la mise à jour de l'opérateur avec la fonction `push!`\n",
    "- si on a pas une direction de descente, alors on skip\n",
    "- recherche linéaire d'Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? LinearOperators.push!\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? LinearOperators.push!\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? LinearOperators.push!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function armijo(xk, dk, fk, gk, slope, nlp :: AbstractNLPModel; τ1 = 1.0e-4, t_update = 1.5)\n",
    "  t = 1.0\n",
    "  fk_new = obj(nlp, xk + dk) # t = 1.0\n",
    "  while fk_new > fk + τ1 * t * slope\n",
    "    t /= t_update\n",
    "    fk_new = obj(nlp, xk + t * dk)\n",
    "  end\n",
    "  return t, fk_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limited_bfgs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function limited_bfgs(nlp      :: AbstractNLPModel;\n",
    "                      x        :: AbstractVector = nlp.meta.x0,\n",
    "                      atol     :: Real = √eps(eltype(x)), \n",
    "                      rtol     :: Real = √eps(eltype(x)),\n",
    "                      max_eval :: Int = -1,\n",
    "                      max_time :: Float64 = 30.0,\n",
    "                      f_min    :: Float64 = -1.0e16,\n",
    "                      verbose  :: Bool = true,\n",
    "                      mem      :: Int = 5)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  xt = zeros(T, n)\n",
    "  ∇ft = zeros(T, n)\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = InverseLBFGSOperator(n) ### Use InverseLBFGSOperator instead of I ###\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "\n",
    "#################################################\n",
    "    d = - H * ∇f ### Compute d\n",
    "#################################################\n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, ft = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    xt = x + t * d\n",
    "    ∇ft = grad(nlp, xt) # grad!(nlp, xt, ∇ft)\n",
    "    s = xt - x\n",
    "    y = ∇ft - ∇f\n",
    "#################################################\n",
    "    push!(H, s, y) ### Update H\n",
    "#################################################\n",
    "\n",
    "    # Move on.\n",
    "    x = xt\n",
    "    f = ft\n",
    "    ∇f = ∇ft\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(\n",
    "        nlp,\n",
    "        status=status,\n",
    "        solution=x,\n",
    "        objective=f,\n",
    "        dual_feas=∇fNorm,\n",
    "        iter=iter,\n",
    "        elapsed_time=elapsed_time,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [3.584428266659278, -1.8481265666485829])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: objective function may be unbounded from below\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire\n",
    "#using Pkg; Pkg.add(\"Test\")\n",
    "using Test\n",
    "using Logging, Test\n",
    "stats = with_logger(NullLogger()) do \n",
    "    limited_bfgs(himmelblau) \n",
    "end\n",
    "@test stats.status == :first_order\n",
    "@test stats.solution ≈ [3.584428266659278, -1.8481265666485827] atol = 1e-6\n",
    "@show (stats.status, stats.solution)\n",
    "stats = with_logger(NullLogger()) do \n",
    "    limited_bfgs(problem2) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [3.584428266659278, -1.8481265666485829])\n",
      "(stats.status, stats.solution) = (:unbounded, [1.29140163e8, 1.0, 1.0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [0.9999999887950609, 0.9999999782159007])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:unbounded, [-975544.6831042847, -764227.9248199855])\n",
      "(stats.status, stats.solution) = (:first_order, [0.9999999962625671, -3.1168150200102845e-9])\n",
      "(stats.status, stats.solution) = (:first_order, [0.99999999073849, -6.617373493448244e-9])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "test set      | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"test set\", Any[], 9, false, false, true, 1.707769640427e9, 1.707769641329e9, false, \"c:\\\\Documents\\\\optimisation\\\\MTH8408-Hiv24\\\\lab3\\\\test-lbfgs\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "include(\"test-lbfgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "- Compare l'implémentation de `limited_bfgs` avec la fonction `lbfgs` qui est disponible dans `JSOSolvers.jl`.\n",
    "- On veut pouvoir tester \"facilement\" plusieurs valeurs de $\\tau$ et du paramètre de mise à jour dans `armijo` sur les problèmes tests. Comment modifier le code pour que ça soit possible?\n",
    "\n",
    "On peut mesurer deux executions de fonctions Julia grâce aux fonctions de `BenchmarkTools.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n? @time\n╙ ── not a unary operator",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1:1\n",
      "? @time\n",
      "╙ ── not a unary operator\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:1"
     ]
    }
   ],
   "source": [
    "? @time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000001 seconds\n",
      "  0.000001 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lbfgs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JSOSolvers\n",
    "@time limited_bfgs\n",
    "@time lbfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: NewtonCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'adapter les méthodes de Newton de façon à résoudre le système linéaire avec une méthode itérative de type gradient conjugué comme suit ($B_k$ représente la matrice hessienne):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LineSearchNewtonCG.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Base.Meta.ParseError",
     "evalue": "ParseError:\n# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:34:4\n    return z\r\nend\n#  └ ── Expected `end`",
     "output_type": "error",
     "traceback": [
      "ParseError:\n",
      "# Error @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:34:4\n",
      "    return z\r\n",
      "end\n",
      "#  └ ── Expected `end`\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:34"
     ]
    }
   ],
   "source": [
    "function cg_optim(H, ∇f)\n",
    "    #setup the tolerance:\n",
    "    n∇f = norm(∇f)\n",
    "#####################################\n",
    "    ϵk = min(0.5, sqrt(norm(∇f))*norm(∇f))\n",
    "####################################\n",
    "    n = length(∇f)\n",
    "    z = zeros(n)\n",
    "    r = ∇f\n",
    "    d = -r\n",
    "    \n",
    "    j = 0\n",
    "    while norm(r) ≥ ϵk && j < 3 * n\n",
    "###############################################\n",
    "        if dot(d, H * d) ≤ 0\n",
    "            if j == 0\n",
    "                return -∇f\n",
    "            else\n",
    "                return z\n",
    "        end\n",
    "##############################################\n",
    "        α = transpose(r)*r/(transpose(d)*H*d)\n",
    "##############################################        \n",
    "        z += α * d\n",
    "        nrr2 = dot(r, r)\n",
    "        r += α * H * d\n",
    "##############################################\n",
    "        β  = transpose(r)*r/nrr2\n",
    "##############################################\n",
    "        d  = -r + β * d\n",
    "        j += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est important ici est qu'on a pas besoin de stocker/évaluer la matrice hessienne entière mais simplement le produit entre la hessienne et un vecteur. Pour un `NLPModels` on utilise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? NLPModels.hprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? NLPModels.hess_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function armijo_Newton_cg(nlp      :: AbstractNLPModel;\n",
    "                          x        :: AbstractVector = nlp.meta.x0,\n",
    "                          atol     :: Real = √eps(eltype(x)), \n",
    "                          rtol     :: Real = √eps(eltype(x)),\n",
    "                          max_eval :: Int = -1,\n",
    "                          max_time :: Float64 = 30.0,\n",
    "                          f_min    :: Float64 = -1.0e16)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = hess_op(nlp, x) # Initialize H as linear operator representing the Hessian matrix\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "        \n",
    "    d = cg_optim(H, ∇f)\n",
    "        \n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, f = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    x += t * d\n",
    "    ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "    H = ### Update H\n",
    "#################################################\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(nlp, status = status, solution=x, objective=f, dual_feas=∇fNorm,\n",
    "                               iter=iter, elapsed_time=elapsed_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer un benchmark\n",
    "\n",
    "On veut maintenant pouvoir réaliser un benchmark de plusieurs solveurs. Pour comparer les algorithmes, il nous faut une collection de problèmes tests et on va utiliser `OptimizationProblems.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez trouver un tutoriel de comment réaliser un benchmark avec ce package sur la documentation [OptimizationProblems.jl/dev/benchmark/](https://juliasmoothoptimizers.github.io/OptimizationProblems.jl/dev/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est fort possible que les petits problèmes tests que l'on résout après l'implémentation ne suffisent pas à déceler des bugs. Mais on peut toujours analyser l'éxecution de notre algorithme sur certains problèmes de la collection afin d'améliorer la valeur de certains paramètres (limite de temps, d'itérations, d'évaluations), détecter un bug, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using PureJuMP.rosenbrock in module Main conflicts with an existing identifier.\n",
      "┌ Info:   iter      f(x)      ‖∇f‖      ∇fᵀd        bk  \n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info:      0   8.4e-01   3.9e+00  -1.6e+01   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      1   2.0e-01   2.4e+00  -1.4e+01   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      2  -8.4e-01   9.9e-01  -4.2e+00   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      3  -1.4e+00   2.4e+00  -7.4e+00   1.3e-01\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      4  -1.6e+00   1.7e+00  -2.8e-01   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      5  -1.7e+00   2.2e-01  -4.2e-03   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      6  -1.7e+00   2.1e-03  -3.9e-07   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      7  -1.7e+00   1.3e-05  -1.5e-11   1.0e+00\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:54\n",
      "┌ Info:      8  -1.7e+00   7.0e-10\n",
      "└ @ Main c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: first-order stationary\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using OptimizationProblems.PureJuMP, NLPModelsJuMP\n",
    "jump_model = AMPGO02() # OptimizationProblems.PureJuMP.AMPGO02\n",
    "prbl = MathOptNLPModel(jump_model)\n",
    "limited_bfgs(prbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous en doutez pour le rapport de cette semaine on va vouloir réaliser une benchmark avec les deux méthodes que l'on a codé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix:\n",
    "\n",
    "Une petite remarque sur la gestion de la mémoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1\n",
      "b = 1\n",
      "b = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour les nombres:\n",
    "a = 1\n",
    "@show a\n",
    "b = a\n",
    "@show b\n",
    "a = 2\n",
    "@show b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0.0, 0.0]\n",
      "(a, b) = ([0.0, 0.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [1.0, 1.0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a, b) = ([2.0, 2.0], [2.0, 2.0])\n",
      "(a, b) = ([1.0, 1.0], [1.0, 1.0])\n",
      "(a, b) = ([2.0, 2.0], [1.0, 1.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.0, 2.0], [1.0, 1.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour les tableaux:\n",
    "a = zeros(Float64, 2) #or zeros(2)\n",
    "@show a\n",
    "b = a\n",
    "@show (a,b)\n",
    "a = ones(Float64, 2)\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = a\n",
    "@show (a,b)\n",
    "a .= 2*ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = copy(a) # or similar(a)\n",
    "@show (a,b)\n",
    "a .= 2 .* ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `nlp` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `nlp` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Documents\\optimisation\\MTH8408-Hiv24\\lab3\\Lab3-notebook.ipynb:2"
     ]
    }
   ],
   "source": [
    "#Pour les NLPModels, il existe aussi des fonctions qui interviennent sur la mémoire\n",
    "gk = grad(nlp, x0)\n",
    "grad!(nlp, x0, gk) #équivaut à gk .= grad(nlp, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez les `grad` et `grad!` à l'aide de `@benchmark` ainsi que:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m13.300 μs\u001b[22m\u001b[39m … \u001b[35m  5.618 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 98.09%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m99.400 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m98.825 μs\u001b[22m\u001b[39m ± \u001b[32m221.971 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m14.87% ±  6.90%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[34m▄\u001b[39m\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m \u001b[39m█\n",
       "  13.3 μs\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       293 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m234.52 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m6\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10000\n",
    "a = rand(n)\n",
    "b = ones(n)\n",
    "c = similar(a)\n",
    "@benchmark c = 2 * a + b * 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark c .= 2 .* a .+ b .* 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particulier, observez la mémoire allouée et le temps moyen requis pour performer la ligne d'instruction.\n",
    "La mémoire nécessaire varie en fonction du nombre et du type d'opération effectué.\n",
    "Parmis les opérations allouant le moins de mémoire, on retrouve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark c .= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour manipuler plus finement la mémoire au cours de votre implémentation, vous pouvez utilisez des fonctions de `LinearAlgebra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? axpy!\n",
    "? axpby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui sont des routines `BLAS`.\n",
    "Faites attention, ces routines ont des effets de bord sur les structures de données `y`.\n",
    "Dès lors, l'utilisation de `@benchmark` accumule successivement les effets de bord, d'où l'utilisation des tests avant `@benchmark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = zeros(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "axpy!(a, x, y)\n",
    "@show y == a * x\n",
    "\n",
    "@benchmark axpy!(a, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = π * ones(n)\n",
    "x = ones(n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n) + b * π * ones(n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela fontionne aussi pour les matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = π * ones(n,n)\n",
    "x = ones(n,n)\n",
    "a = 2\n",
    "b = 3\n",
    "axpby!(a, x, b, y)\n",
    "@show y == a * ones(n,n) + b * π * ones(n,n)\n",
    "\n",
    "@benchmark axpby!(a, x, b, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
